{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully retrieved from server.\n",
      "\n",
      "First 5 rows of data:\n",
      "  status message                                               json\n",
      "0    200      ok  {\"index\":{\"0\":0,\"1\":1,\"2\":2,\"3\":3,\"4\":4,\"5\":5,...\n",
      "\n",
      "Data saved as 'dataset.csv'.\n",
      "\n",
      "No complex columns found. No unpacking needed.\n",
      "\n",
      "Data is prepared and ready for further processing.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename=\"parser_errors.log\",\n",
    "    level=logging.ERROR,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# Load configuration\n",
    "try:\n",
    "    with open(\"config.json\", \"r\", encoding=\"utf-8\") as config_file:\n",
    "        config = json.load(config_file)\n",
    "        url = config.get(\"url\")\n",
    "        token = config.get(\"token\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading configuration: {e}\")\n",
    "    print(\"Error loading 'config.json'.\")\n",
    "    exit()\n",
    "\n",
    "# HTTP headers\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {token}\"\n",
    "}\n",
    "\n",
    "# Fetch data from server\n",
    "try:\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    print(\"Data successfully retrieved from server.\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    logging.error(f\"HTTP request error: {e}\")\n",
    "    print(\"Error retrieving data.\")\n",
    "    exit()\n",
    "\n",
    "# Parse JSON content directly from memory\n",
    "try:\n",
    "    data = response.json()\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error parsing JSON response: {e}\")\n",
    "    print(\"Error parsing JSON response.\")\n",
    "    exit()\n",
    "\n",
    "# Convert to DataFrame\n",
    "try:\n",
    "    if isinstance(data, list):\n",
    "        df = pd.DataFrame(data)\n",
    "    elif isinstance(data, dict):\n",
    "        df = pd.json_normalize(data)\n",
    "    else:\n",
    "        raise Exception(\"Unsupported data format.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error converting to DataFrame: {e}\")\n",
    "    print(\"Unsupported data format.\")\n",
    "    exit()\n",
    "\n",
    "# Display first 5 rows\n",
    "print(\"\\nFirst 5 rows of data:\")\n",
    "print(df.head(5))\n",
    "\n",
    "# Save to CSV\n",
    "try:\n",
    "    df.to_csv(\"dataset.csv\", index=False, encoding=\"utf-8\")\n",
    "    print(\"\\nData saved as 'dataset.csv'.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error saving CSV file: {e}\")\n",
    "    print(\"Error saving CSV file.\")\n",
    "\n",
    "# Identify complex columns\n",
    "def is_complex_type(elem):\n",
    "    return isinstance(elem, (dict, list))\n",
    "\n",
    "def is_list(elem):\n",
    "    return isinstance(elem, list)\n",
    "\n",
    "try:\n",
    "    complex_columns = [col for col in df.columns if df[col].map(is_complex_type).any()]\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error identifying complex columns: {e}\")\n",
    "    complex_columns = []\n",
    "\n",
    "# Unpack complex columns\n",
    "if complex_columns:\n",
    "    print(\"\\nFound complex columns:\", complex_columns)\n",
    "\n",
    "    for col in complex_columns:\n",
    "        try:\n",
    "            print(f\"\\nUnpacking column: {col}\")\n",
    "            col_non_null = df[col].dropna()\n",
    "            if col_non_null.map(is_list).all():\n",
    "                # Explode if all values are lists\n",
    "                exploded = df[[col]].explode(col)\n",
    "                nested_df = pd.json_normalize(exploded[col])\n",
    "            else:\n",
    "                # Otherwise assume dicts\n",
    "                nested_df = pd.json_normalize(col_non_null)\n",
    "\n",
    "            nested_df.to_csv(f\"{col}_table.csv\", index=False, encoding=\"utf-8\")\n",
    "            print(f\"Column '{col}' unpacked and saved as '{col}_table.csv'.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing column '{col}': {e}\")\n",
    "            print(f\"Error processing column: '{col}'. See 'parser_errors.log'.\")\n",
    "else:\n",
    "    print(\"\\nNo complex columns found. No unpacking needed.\")\n",
    "\n",
    "print(\"\\nData is prepared and ready for further processing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to the database!\n",
      "Table 'transactions' has been cleared.\n",
      "Detected JSON in 'json' column. Unpacking first row only for simplicity.\n",
      "Inserted 1000 rows...\n",
      "All data inserted successfully.\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "server = \"localhost\\\\MSSQLSERVER01\"\n",
    "database = \"bank\"\n",
    "conn_str = f'DRIVER={{SQL Server}};SERVER={server};DATABASE={database};Trusted_Connection=yes;'\n",
    "\n",
    "try:\n",
    "    conn = pyodbc.connect(conn_str)\n",
    "    cursor = conn.cursor()\n",
    "    print(\"Successfully connected to the database!\")\n",
    "\n",
    "    # Ensure the table exists, otherwise create it\n",
    "    create_table_query = \"\"\"\n",
    "    IF OBJECT_ID('dbo.transactions', 'U') IS NULL\n",
    "    CREATE TABLE transactions (\n",
    "        id INT IDENTITY(1,1) PRIMARY KEY,\n",
    "        city NVARCHAR(255),\n",
    "        date DATE,\n",
    "        card_type NVARCHAR(50),\n",
    "        exp_type NVARCHAR(50),\n",
    "        gender CHAR(1),\n",
    "        amount INT\n",
    "    )\n",
    "    \"\"\"\n",
    "    cursor.execute(create_table_query)\n",
    "    conn.commit()\n",
    "\n",
    "    # Truncate the table to remove old data\n",
    "    cursor.execute(\"TRUNCATE TABLE dbo.transactions;\")\n",
    "    conn.commit()\n",
    "    print(\"Table 'transactions' has been cleared.\")\n",
    "\n",
    "    # Prepare insert query\n",
    "    insert_query = \"\"\"\n",
    "    INSERT INTO transactions (city, date, card_type, exp_type, gender, amount)\n",
    "    VALUES (?, ?, ?, ?, ?, ?)\n",
    "    \"\"\"\n",
    "\n",
    "    # Read CSV in chunks\n",
    "    chunks = pd.read_csv(\"dataset.csv\", sep=\",\", encoding=\"utf-8-sig\", chunksize=1000)\n",
    "\n",
    "    for chunk in chunks:\n",
    "        # Optional JSON unpacking (if needed)\n",
    "        if \"json\" in chunk.columns:\n",
    "            print(\"Detected JSON in 'json' column. Unpacking first row only for simplicity.\")\n",
    "            unpacked = json.loads(chunk[\"json\"].iloc[0])\n",
    "            chunk = pd.DataFrame(unpacked)\n",
    "\n",
    "        # Normalize column names\n",
    "        chunk.columns = chunk.columns.str.strip().str.lower()\n",
    "\n",
    "        # Convert data types\n",
    "        chunk[\"date\"] = chunk[\"date\"].astype(str)\n",
    "        chunk[\"amount\"] = chunk[\"amount\"].astype(int)\n",
    "\n",
    "        # Prepare values directly from chunk (generator)\n",
    "        values = zip(\n",
    "            chunk[\"city\"],\n",
    "            chunk[\"date\"],\n",
    "            chunk[\"card type\"],\n",
    "            chunk[\"exp type\"],\n",
    "            chunk[\"gender\"],\n",
    "            chunk[\"amount\"]\n",
    "        )\n",
    "\n",
    "        cursor.executemany(insert_query, list(values))  # or just values if pyodbc supports it\n",
    "        conn.commit()\n",
    "        print(\"Inserted 1000 rows...\")\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    print(\"All data inserted successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Database connection error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_17272\\2823864497.py:9: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(\"SELECT * FROM transactions\", conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'new_transactions' has been cleared using TRUNCATE.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'city'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'city'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 45\u001b[0m\n\u001b[0;32m     38\u001b[0m insert_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124mINSERT INTO new_transactions (city, country, date, card_type, exp_type, gender, amount)\u001b[39m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124mVALUES (?, ?, ?, ?, ?, ?, ?)\u001b[39m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Prepare data for batch insert\u001b[39;00m\n\u001b[0;32m     44\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m---> 45\u001b[0m     \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcity\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[0;32m     46\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcountry\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     47\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     48\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcard_type\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     49\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexp_type\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     50\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgender\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     51\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamount\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     52\u001b[0m ))\n\u001b[0;32m     54\u001b[0m cursor\u001b[38;5;241m.\u001b[39mexecutemany(insert_query, values)\n\u001b[0;32m     55\u001b[0m conn\u001b[38;5;241m.\u001b[39mcommit()\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'city'"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "\n",
    "# Connection config\n",
    "conn = pyodbc.connect(\"DRIVER={SQL Server};SERVER=localhost\\\\MSSQLSERVER01;DATABASE=bank;Trusted_Connection=yes;\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Read original data\n",
    "df = pd.read_sql(\"SELECT * FROM transactions\", conn)\n",
    "\n",
    "# Split 'city' into 'city' and 'country'\n",
    "df[['city', 'country']] = df['city'].str.split(', ', expand=True)\n",
    "\n",
    "# Ensure new table exists, otherwise create it\n",
    "create_table_query = \"\"\"\n",
    "IF OBJECT_ID('dbo.new_transactions', 'U') IS NULL\n",
    "CREATE TABLE new_transactions (\n",
    "    id INT IDENTITY(1,1) PRIMARY KEY,\n",
    "    city NVARCHAR(255),\n",
    "    country NVARCHAR(255),\n",
    "    date DATE,\n",
    "    card_type NVARCHAR(50),\n",
    "    exp_type NVARCHAR(50),\n",
    "    gender CHAR(1),\n",
    "    amount INT\n",
    ")\n",
    "\"\"\"\n",
    "cursor.execute(create_table_query)\n",
    "conn.commit()\n",
    "\n",
    "# Truncate the table to keep structure but remove old data\n",
    "cursor.execute(\"TRUNCATE TABLE new_transactions;\")\n",
    "conn.commit()\n",
    "print(\"Table 'new_transactions' has been cleared using TRUNCATE.\")\n",
    "\n",
    "# Prepare insert query\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO new_transactions (city, country, date, card_type, exp_type, gender, amount)\n",
    "VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "\"\"\"\n",
    "\n",
    "# Prepare data for batch insert\n",
    "values = list(zip(\n",
    "    df['city'],\n",
    "    df['country'],\n",
    "    df['date'],\n",
    "    df['card_type'],\n",
    "    df['exp_type'],\n",
    "    df['gender'],\n",
    "    df['amount']\n",
    "))\n",
    "\n",
    "cursor.executemany(insert_query, values)\n",
    "conn.commit()\n",
    "print(\"Data successfully inserted into 'new_transactions'.\")\n",
    "\n",
    "# Cleanup\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
